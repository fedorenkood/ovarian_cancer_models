{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from oop_functions.classifier_oop import *\n",
    "from oop_functions.util_functions import *\n",
    "from oop_functions.merge_dataset_functions import *\n",
    "from oop_functions.analytics_oop import *\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_data = pd.read_csv('./processed_dataset/recent_propagated_dataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "year = 1\n",
    "label = f'cancer_in_next_{year}_years'\n",
    "source_df = processed_data\n",
    "source_df = resample_class(source_df, label, 0, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(eval_metric= 'error', learning_rate= 0.1)\n",
    "experiment_data_helper = ExperimentDataHelper1(source_df, label, ['cancer_'])\n",
    "data_util_lambdas = experiment_data_helper.train_test_split_util.split_kfold(5)\n",
    "data_util = data_util_lambdas[0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "analytics_util = AnalyticsUtil(classifier, data_util).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "analytics_util.get_report_generation_util().display_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get cv split\n",
    "# run cv classifiers\n",
    "# aggregate cv results\n",
    "# visualize cv results\n",
    "# save cv results\n",
    "class CvAnalyticsUtil:\n",
    "    def __init__(self, analytics_utils: List[AnalyticsUtil]) -> None:\n",
    "        self.analytics_utils = analytics_utils\n",
    "\n",
    "    def get_cv_report(self):\n",
    "        cv_scores = []\n",
    "        for k, analytics_util in enumerate(self.analytics_utils):\n",
    "            report = analytics_util.get_report_generation_util().generate_report()\n",
    "            cv_scores.append(report)\n",
    "        cv_scores = pd.concat(cv_scores)\n",
    "        cv_scores = cv_scores.reset_index()\n",
    "        cv_scores = cv_scores.drop('index', axis=1)\n",
    "        measures_df = cv_scores.describe().T[['mean', 'std', 'min', 'max']]\n",
    "        print('\\n\\nCross-Validation measures:')\n",
    "        print_df(measures_df)\n",
    "        return cv_scores, measures_df\n",
    "\n",
    "    def calc_confusion_matrix(self):\n",
    "        pass\n",
    "\n",
    "    def roc_with_interval(self):\n",
    "        pass\n",
    "\n",
    "    def precision_recall_with_intervals(self):\n",
    "        pass\n",
    "\n",
    "class FeatureImportanceCvAnalyticsUtil(CvAnalyticsUtil):\n",
    "    def get_cv_feature_selection(self):\n",
    "        df_feature_importance_tree = None\n",
    "        for k, analytics_util in enumerate(self.analytics_utils):\n",
    "            fn=analytics_util.data_util.get_feature_names()\n",
    "            top_feature_stats, feature_importances = analytics_util.feature_selection()\n",
    "            feature_importances = feature_importances[feature_importances['importance'] > 0]\n",
    "            if df_feature_importance_tree is not None:\n",
    "                df_feature_importance_tree = df_feature_importance_tree.merge(feature_importances, on='column_name', how='outer', suffixes=[f'_tiral_{k}', f'_tiral_{k+1}'])\n",
    "            else:\n",
    "                df_feature_importance_tree = feature_importances\n",
    "        # Mean of feature importance over trials\n",
    "        df_feature_importance_mean = df_feature_importance_tree.drop('column_name', axis=1)\n",
    "        df_feature_importance_mean = df_feature_importance_mean.T\n",
    "        df_feature_importance_mean.columns = df_feature_importance_tree['column_name']\n",
    "        df_feature_importance_mean = df_feature_importance_mean.astype('float')\n",
    "        df_feature_importance_mean_describe = df_feature_importance_mean.describe().T\n",
    "        df_feature_importance_mean_describe.sort_values('mean', ascending=False, inplace=True)\n",
    "        # print(df_feature_importance_mean_describe.columns)\n",
    "        df_feature_importance_mean_describe = df_feature_importance_mean_describe[['count', 'mean']]\n",
    "        # print_df(df_feature_importance_mean_describe)\n",
    "        df_feature_importance_mean_describe = df_feature_importance_mean_describe.merge(missing_df, on='column_name')\n",
    "        return df_feature_importance_mean_describe\n",
    "        \n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, \n",
    "                 classifier, \n",
    "                 experiment_data_helper: ExperimentDataHelper, \n",
    "                 num_folds: int = 10, \n",
    "                 test_n_folds: int = 1, \n",
    "                 n_repeats: int = 1, \n",
    "                 display: bool = False) -> None:\n",
    "        self.classifier = clone(classifier) \n",
    "        self.experiment_data_helper = experiment_data_helper\n",
    "        self.display = display\n",
    "        # Instead of running all folds just run several folds so that it is easier to bench test\n",
    "        self.num_folds = num_folds\n",
    "        self.test_n_folds = test_n_folds\n",
    "        if self.test_n_folds is None:\n",
    "            self.test_n_folds = self.num_folds * self.n_repeats\n",
    "        # TODO: RepeatedStratifiedKFold instead of StratifiedKFold to run experiments\n",
    "        self.n_repeats = n_repeats\n",
    "        self.cv_analytics_util = None\n",
    "\n",
    "    # TODO: make these ones polymorphic\n",
    "    def get_analytics_util(self) -> AnalyticsUtil:\n",
    "        return AnalyticsUtil\n",
    "\n",
    "    def get_cv_analytics_util(self) -> CvAnalyticsUtil:\n",
    "        return CvAnalyticsUtil\n",
    "\n",
    "    def run_experiment(self) -> ExperimentRunner:\n",
    "        data_util_lambdas = experiment_data_helper.train_test_split_util.split_kfold(self.num_folds)\n",
    "        analytics_utils = []\n",
    "        for i in range(self.test_n_folds):\n",
    "            data_util = data_util_lambdas[0]\n",
    "            analytics_util = self.get_analytics_util()(self.classifier, data_util).fit()\n",
    "            analytics_utils.append(analytics_util)\n",
    "        self.cv_analytics_util = self.get_cv_analytics_util()(analytics_utils)\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scale_features(df):\n",
    "    sc = StandardScaler()\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled = sc.fit_transform(df_scaled)\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=df.columns, index=df.index)\n",
    "    return df_scaled\n",
    "\n",
    "def get_nearest_neighbors(df1, df2, top=5):\n",
    "    df1 = df1.drop_duplicates()\n",
    "    df2 = df2.drop_duplicates()\n",
    "    df1 = scale_features(df1)\n",
    "    df2 = scale_features(df2)\n",
    "    euclidean_distances = []\n",
    "    indexes = []\n",
    "    for i in range(len(df1)):\n",
    "        row1 = df1.iloc[i]\n",
    "        distances = []\n",
    "        for j, row2 in df2.iterrows():\n",
    "            distances.append((j, distance.euclidean(row1, row2)))\n",
    "        distances = sorted(distances, key=lambda x: x[1], reverse=False)[:top]\n",
    "        distances = pd.DataFrame(distances, columns=['index', 'distance'])\n",
    "        indexes.append((distances['index'].to_list()))\n",
    "        euclidean_distances.append(distances['distance'].to_list())\n",
    "    return euclidean_distances, indexes\n",
    "\n",
    "def get_high_confidence_errors(classifier, X_train, X_test, y_train, y_test, label, label_val=0):\n",
    "    # Insert predicted class and its likelihood\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_prob = classifier.predict_proba(X_test)[:,1]\n",
    "    X_test_mismatch = X_test.copy()\n",
    "    X_test_mismatch[label] = y_test\n",
    "    X_test_mismatch[f'{label}_pred'] = y_pred\n",
    "    X_test_mismatch[f'{label}_prob'] = y_prob\n",
    "    X_test_mismatch = X_test_mismatch.drop_duplicates()\n",
    "    X_test_mismatch = X_test_mismatch[X_test_mismatch[label] != X_test_mismatch[f'{label}_pred']]\n",
    "    \n",
    "    # X_test_high_conf = X_test_mismatch[(X_test_mismatch[f'{label}_prob'] < 0.2) | (X_test_mismatch[f'{label}_prob'] > 0.8)]\n",
    "    X_test_high_conf = X_test_mismatch\n",
    "    X_test_high_conf = X_test_high_conf[X_test_high_conf[f'{label}_pred'] == label_val]\n",
    "    \n",
    "    # Select 5 nearest neightbors \n",
    "    X_train[label] = y_train\n",
    "    X_train_filtered = X_train[X_train[label] == label_val].drop(label, axis=1)\n",
    "    # X_test_high_conf = X_test.loc[X_test_high_conf.index, :]\n",
    "    # Calculated euclidean distances\n",
    "    distances, indices = get_nearest_neighbors(X_test.loc[X_test_high_conf.index, :], X_train_filtered)\n",
    "    fp_mismatches = []\n",
    "    X_train[f'{label}_pred'] = -1\n",
    "    X_train[f'{label}_prob'] = -1\n",
    "    X_train = X_train.drop_duplicates()\n",
    "    # print_df(X_train)\n",
    "    for i in range(len(X_test_high_conf)):\n",
    "        idx = indices[i]\n",
    "        missed_record = X_test_high_conf.iloc[[i], :]\n",
    "        missed_record['distance'] = 0\n",
    "        close_records = X_train.loc[idx, :]\n",
    "        close_records['distance'] = distances[i]\n",
    "        fp_mismatches.append((missed_record, close_records))\n",
    "    return fp_mismatches\n",
    "\n",
    "def run_classifiers(classifiers, df, label, strategy='median', drop_n_features = 1, num_folds=10, cv=False, plot_tree=False, differentiate_confusion_matrix_over=None):\n",
    "    df_feature_importance_tree = None\n",
    "    df_top_feature_importances = []\n",
    "    for i in range(drop_n_features):\n",
    "        \n",
    "        train_test_lambda, k_fold_lambdas = process_and_impute_for_label_kfold(df, label, strategy, n_max_per_class=10000, num_folds=num_folds, differentiate_confusion_matrix_over=differentiate_confusion_matrix_over)\n",
    "        X_train, X_test, y_train, y_test, differentiated_test_sets = train_test_lambda()\n",
    "        auc_dict = {}\n",
    "        accuracy_dict = {}\n",
    "        top_feature_importance = []\n",
    "        for classifier_type, classifier in classifiers.items():\n",
    "            # Cross Validation\n",
    "            if cv:\n",
    "                cv_scores, measures_df, df_feature_importance_mean_describe = get_cv_scores(classifier, k_fold_lambdas)\n",
    "                cv_scores.to_csv(f'./cv_scores/cv_scores_for_{classifier_type}_{label}.csv')\n",
    "                measures_df.to_csv(f'./cv_scores/cv_stats_for_{classifier_type}_{label}.csv')\n",
    "                if classifier_type in ['DecisionTreeClassifier', 'XGBClassifier']:\n",
    "                    df_feature_importance_mean_describe.to_csv(f'./feature_importance/feature_importance_mean_{classifier_type}_{label}__{num_folds}_trials.csv')\n",
    "\n",
    "            # Test classifier\n",
    "            print(classifier_type)\n",
    "            trained_classifier, auc, accuracy, threshold, report = run_classifier(classifier, X_train, X_test, y_train, y_test)\n",
    "            print(threshold)\n",
    "            filtered_on = list(itertools.chain.from_iterable([zip([key]*len(vals), vals) for key, vals in differentiate_confusion_matrix_over.items()]))\n",
    "            for i, (X_test_filtered, y_test_filtered) in enumerate(differentiated_test_sets):\n",
    "                print(f'Filtered on: {filtered_on[i]}')                \n",
    "                y_pred = trained_classifier.predict(X_test_filtered)\n",
    "                y_prob = trained_classifier.predict_proba(X_test_filtered)[:,1]\n",
    "                performance_analysis(y_pred, y_prob, y_test_filtered, show_graph=True)\n",
    "\n",
    "            # Decision Tree feature selection\n",
    "            if classifier_type in ['DecisionTreeClassifier', 'XGBClassifier']:\n",
    "                fn=X_train.columns\n",
    "                top_feature_stats, feature_importances = feature_selection(classifier, fn, accuracy, plot_tree=plot_tree,\n",
    "                                                                                         filepath=f'./trees/decision_tree_for_{classifier_type}_{label}.png')\n",
    "                df_top_feature_importances.append(top_feature_stats)\n",
    "                top_feature_importance.append(feature_importances.iloc[0]['column_name'])\n",
    "                feature_importances = feature_importances[feature_importances['importance'] > 0]\n",
    "                if df_feature_importance_tree is not None:\n",
    "                    df_feature_importance_tree = df_feature_importance_tree.merge(feature_importances, on='column_name', how='outer', suffixes=[f'_tiral_{i}', f'_tiral_{i+1}'])\n",
    "                else:\n",
    "                    df_feature_importance_tree = feature_importances\n",
    "            auc_dict[classifier_type] = auc\n",
    "            accuracy_dict[classifier_type] = accuracy\n",
    "            \n",
    "            fp_mismatches = get_high_confidence_errors(classifier, X_train, X_test, y_train, y_test, label, label_val=0)\n",
    "        # df = df.drop(top_feature_importance, axis=1)\n",
    "\n",
    "    return df_top_feature_importances, df_feature_importance_tree, fp_mismatches\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
